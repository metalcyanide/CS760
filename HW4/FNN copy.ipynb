{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(fp):\n",
    "    with open(fp, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    return np.frombuffer(data, dtype=np.uint8).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fetch(\"./mnist/train-images-idx3-ubyte\")[0x10:].reshape((-1, 28 * 28))\n",
    "Y_train = fetch(\"./mnist/train-labels-idx1-ubyte\")[8:]\n",
    "X_test = fetch(\"./mnist/t10k-images-idx3-ubyte\")[0x10:].reshape((-1, 28 * 28))\n",
    "Y_test = fetch(\"./mnist/t10k-labels-idx1-ubyte\")[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_oneHotEncoding(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    oneHotEncoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    Y_train = oneHotEncoder.fit_transform(Y_train.reshape(len(Y_train), -1))\n",
    "    Y_test = oneHotEncoder.transform(Y_test.reshape(len(Y_test), -1))\n",
    "    \n",
    "    # Normalize data by diving by 255. All values are in range 0-255\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MCN/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = normalize_oneHotEncoding(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights():\n",
    "    W1 = np.random.randn(300, 784) / np.sqrt(784)\n",
    "    W2 = np.random.randn(10, 300) / np.sqrt(300)\n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    val = np.exp(x - np.max(x))\n",
    "    return val / val.sum(axis=0)\n",
    "\n",
    "def de_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    z1 = w1.dot(X.T)\n",
    "    i1 = sigmoid(z1)\n",
    "    z2 = w2.dot(i1)\n",
    "    i2 = softmax(z2)\n",
    "    return i1, w1, z1, i2, w2, z2\n",
    "\n",
    "\n",
    "\n",
    "def backward(X, Y, i1, w1, z1, i2, w2, z2, n=60000):\n",
    "    dz2 = i2 - Y.T\n",
    "    dw2 = dz2.dot(i1.T) / n\n",
    "    dz1 = w2.T.dot(dz2) * de_sigmoid(z1)\n",
    "    dw1 = dz1.dot(X) / n\n",
    "    return dw1, dw2\n",
    "\n",
    "def predict(w1, w2, X, Y):\n",
    "    i1, w1, z1, i2, w2, z2 = forward(X, w1, w2)\n",
    "    y_hat = np.argmax(i2, axis=0)\n",
    "    Y = np.argmax(Y, axis = 1)\n",
    "    accuracy = (y_hat == Y).mean()\n",
    "    return accuracy * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "accus = []\n",
    "\n",
    "\n",
    "def train_model(X, Y, alpha=0.1, epochs=1000):\n",
    "    w1, w2 = init_weights()\n",
    "    for epoch in range(epochs):\n",
    "        i1, w1, z1, i2, w2, z2 = forward(X, w1, w2)\n",
    "        cost = -np.mean(Y*np.log(i2.T))\n",
    "        dw1, dw2 = backward(X, Y, i1, w1, z1, i2, w2, z2)\n",
    "        w1 = w1 - alpha * dw1\n",
    "        w2 = w2 - alpha * dw2\n",
    "        acc = predict(w1, w2, X, Y)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Cost: \", cost, \"Train Accuracy:\", acc)\n",
    "        if epoch % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            accus.append(acc)\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.24037718898628085 Train Accuracy: 11.375\n",
      "Cost:  0.14360326597747294 Train Accuracy: 74.59166666666667\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d1, d2):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, d1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(d1, d2),\n",
    "            nn.Softmax(),\n",
    "            nn.Linear(d2, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN(300, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "bs = 64\n",
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 23.009\n",
      "[2,  1000] loss: 23.009\n",
      "[3,  1000] loss: 23.009\n",
      "[4,  1000] loss: 23.009\n",
      "[5,  1000] loss: 23.009\n",
      "[6,  1000] loss: 23.009\n",
      "[7,  1000] loss: 23.009\n",
      "[8,  1000] loss: 23.009\n",
      "[9,  1000] loss: 23.009\n",
      "[10,  1000] loss: 23.009\n",
      "[11,  1000] loss: 23.009\n",
      "[12,  1000] loss: 23.009\n",
      "[13,  1000] loss: 23.009\n",
      "[14,  1000] loss: 23.009\n",
      "[15,  1000] loss: 23.009\n",
      "[16,  1000] loss: 23.009\n",
      "[17,  1000] loss: 23.009\n",
      "[18,  1000] loss: 23.009\n",
      "[19,  1000] loss: 23.009\n",
      "[20,  1000] loss: 23.009\n",
      "[21,  1000] loss: 23.009\n",
      "[22,  1000] loss: 23.009\n",
      "[23,  1000] loss: 23.009\n",
      "[24,  1000] loss: 23.009\n",
      "[25,  1000] loss: 23.009\n",
      "[26,  1000] loss: 23.009\n",
      "[27,  1000] loss: 23.009\n",
      "[28,  1000] loss: 23.009\n",
      "[29,  1000] loss: 23.009\n",
      "[30,  1000] loss: 23.009\n",
      "[31,  1000] loss: 23.009\n",
      "[32,  1000] loss: 23.009\n",
      "[33,  1000] loss: 23.009\n",
      "[34,  1000] loss: 23.009\n",
      "[35,  1000] loss: 23.009\n",
      "[36,  1000] loss: 23.009\n",
      "[37,  1000] loss: 23.009\n",
      "[38,  1000] loss: 23.009\n",
      "[39,  1000] loss: 23.009\n",
      "[40,  1000] loss: 23.009\n",
      "[41,  1000] loss: 23.009\n",
      "[42,  1000] loss: 23.009\n",
      "[43,  1000] loss: 23.009\n",
      "[44,  1000] loss: 23.009\n",
      "[45,  1000] loss: 23.009\n",
      "[46,  1000] loss: 23.009\n",
      "[47,  1000] loss: 23.009\n",
      "[48,  1000] loss: 23.009\n",
      "[49,  1000] loss: 23.009\n",
      "[50,  1000] loss: 23.009\n",
      "[51,  1000] loss: 23.009\n",
      "[52,  1000] loss: 23.009\n",
      "[53,  1000] loss: 23.009\n",
      "[54,  1000] loss: 23.009\n",
      "[55,  1000] loss: 23.009\n",
      "[56,  1000] loss: 23.009\n",
      "[57,  1000] loss: 23.009\n",
      "[58,  1000] loss: 23.008\n",
      "[59,  1000] loss: 23.008\n",
      "[60,  1000] loss: 23.008\n",
      "[61,  1000] loss: 23.008\n",
      "[62,  1000] loss: 23.008\n",
      "[63,  1000] loss: 23.008\n",
      "[64,  1000] loss: 23.008\n",
      "[65,  1000] loss: 23.008\n",
      "[66,  1000] loss: 23.008\n",
      "[67,  1000] loss: 23.008\n",
      "[68,  1000] loss: 23.008\n",
      "[69,  1000] loss: 23.008\n",
      "[70,  1000] loss: 23.008\n",
      "[71,  1000] loss: 23.008\n",
      "[72,  1000] loss: 23.008\n",
      "[73,  1000] loss: 23.008\n",
      "[74,  1000] loss: 23.008\n",
      "[75,  1000] loss: 23.008\n",
      "[76,  1000] loss: 23.008\n",
      "[77,  1000] loss: 23.008\n",
      "[78,  1000] loss: 23.008\n",
      "[79,  1000] loss: 23.008\n",
      "[80,  1000] loss: 23.008\n",
      "[81,  1000] loss: 23.008\n",
      "[82,  1000] loss: 23.008\n",
      "[83,  1000] loss: 23.008\n",
      "[84,  1000] loss: 23.008\n",
      "[85,  1000] loss: 23.008\n",
      "[86,  1000] loss: 23.008\n",
      "[87,  1000] loss: 23.008\n",
      "[88,  1000] loss: 23.008\n",
      "[89,  1000] loss: 23.008\n",
      "[90,  1000] loss: 23.008\n",
      "[91,  1000] loss: 23.008\n",
      "[92,  1000] loss: 23.008\n",
      "[93,  1000] loss: 23.008\n",
      "[94,  1000] loss: 23.008\n",
      "[95,  1000] loss: 23.008\n",
      "[96,  1000] loss: 23.008\n",
      "[97,  1000] loss: 23.008\n",
      "[98,  1000] loss: 23.008\n",
      "[99,  1000] loss: 23.008\n",
      "[100,  1000] loss: 23.008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 16 test images: 11 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(images)} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0.000000 is 0.0 %\n",
      "Accuracy for class: 1.000000 is 100.0 %\n",
      "Accuracy for class: 2.000000 is 0.0 %\n",
      "Accuracy for class: 3.000000 is 0.0 %\n",
      "Accuracy for class: 4.000000 is 0.0 %\n",
      "Accuracy for class: 5.000000 is 0.0 %\n",
      "Accuracy for class: 6.000000 is 0.0 %\n",
      "Accuracy for class: 7.000000 is 0.0 %\n",
      "Accuracy for class: 8.000000 is 0.0 %\n",
      "Accuracy for class: 9.000000 is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "classes = list(range(10))\n",
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5f} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
